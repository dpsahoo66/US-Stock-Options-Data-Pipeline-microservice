x-airflow-common:
  &airflow-common
    build:
      context: .
      dockerfile: Dockerfile-airflow
    environment:
      &airflow-common-env
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      AIRFLOW__CORE__SQL_ALCHEMY_POOL_ENABLED: 'True'
      AIRFLOW__CORE__SQL_ALCHEMY_POOL_SIZE: 5
      AIRFLOW__CORE__SQL_ALCHEMY_MAX_OVERFLOW: 10
      AIRFLOW__CORE__SQL_ALCHEMY_POOL_RECYCLE: 1800
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./ca.pem:/ca.pem
    user: "${AIRFLOW_UID:-50000}:0"
    depends_on:
      kafka:
        condition: service_started
services:
  airflow:
    <<: *airflow-common
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    entrypoint: ["/entrypoint.sh"]
    networks:
      - stock_pipeline

  load-balancer:
    image: nginx:latest
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "80:80"
    depends_on:
      - data-collector
    networks:
      - stock_pipeline

  data-collector:
    build:
      context: ./data_collector_service
      dockerfile: Dockerfile
    environment:
      - TWELVE_DATA_API_1=<enter_api_key1_here>
      - TWELVE_DATA_API_2=<enter_api_key2_here>
      - TWELVE_DATA_API_3=<enter_api_key3_here>
      - TWELVE_DATA_API_4=<enter_api_key4_here>
      - TWELVE_DATA_API_5=<enter_api_key5_here>

      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TRIGGER_DAILY=fetch-trigger-daily
      - KAFKA_TRIGGER_15MIN=fetch-trigger-15min
      - KAFKA_TRIGGER_HISTORICAL=fetch-trigger-historical
      - KAFKA_DAILY_TOPIC=daily-data
      - KAFKA_15min_TOPIC=15min-data
      - KAFKA_OPTION_TOPIC=options-data
      - KAFKA_HISTORICAL_TOPIC=historical-data
      
      - DJANGO_SECRET_KEY="django-insecure-k^c404!2*woj(h(+ek*e#=0sh^qrjw9y-g34m^*qy=^=!43v%^"
      - APP_DEBUG=True
    ports:
      - "8000:8000"
    depends_on:
      - kafka
    networks:
      - stock_pipeline
  
  data-processor:
    build:
      context: ./data_processor_service
      dockerfile: Dockerfile
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_DAILY_TOPIC=daily-data
      - KAFKA_15min_TOPIC=15min-data
      - KAFKA_OPTION_TOPIC=options-data
      - KAFKA_HISTORICAL_TOPIC=historical-data
      - KAFKA_PROCESSED_DAILY=processed-daily-data
      - KAFKA_PROCESSED_15MIN=processed-15min-data
      - KAFKA_PROCESSED_OPTIONS=processed-options-data
      - KAFKA_PROCESSED_HISTORICAL=processed-historical-data
      - KAFKA_PROCESSED_FILE_DAILY=processed-file-daily-data
      - KAFKA_PROCESSED_FILE_15MIN=processed-file-15min-data
      - KAFKA_PROCESSED_FILE_OPTIONS=processed-file-options-data
      - KAFKA_PROCESSED_FILE_HISTORICAL=processed-file-historical-data

      - DJANGO_SECRET_KEY=django-insecure-#2k^ti5^bcw_ce5-73!ce&w6ywg)6(_9gpceh+%!nqa^yp1=r6
      - APP_DEBUG=True
    ports:
      - "8001:8001"
    depends_on:
      - kafka
    networks:
      - stock_pipeline

  database-writer:
    build:
      context: ./database_writer_service
      dockerfile: Dockerfile
    environment:
      - AZURE_SQL_CONNECTION_STRING=${AZURE_SQL_CONNECTION_STRING}

      - INFLUXDB_URL=https://us-east-1-1.aws.cloud2.influxdata.com
      - INFLUXDB_TOKEN=p4vPDiJyynco8tjaNhG2ch7A51SHtzN0ta3VsJ6Y1OqVyHtvuAL7K_gKOVmsYV47F_hqaNPlHKOdi6Y_C8Xjw==
      - INFLUXDB_ORG=US_Stock_Data
      - INFLUXDB_BUCKET=fifteenmin_stockdata

      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_PROCESSED_DAILY=processed-daily-data
      - KAFKA_PROCESSED_15MIN=processed-15min-data
      - KAFKA_PROCESSED_OPTIONS=processed-options-data
      - KAFKA_PROCESSED_HISTORICAL=processed-historical-data
      
      - DJANGO_SECRET_KEY=django-insecure-np#5k+3!h2y5he0r=inoh*l5^nw*!t-&e^t-p5zb5ror*^jkyw
      - APP_DEBUG=True
      - RUN_DB_HANDLER=False

    ports:
      - "8002:8002"
    depends_on:
      - kafka
    networks:
      - stock_pipeline

  file-writer:
    build:
      context: ./file_writer_service    
      dockerfile: Dockerfile
    environment:
      - AZURE_SQL_CONNECTION_STRING=${AZURE_SQL_CONNECTION_STRING}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - S3_BUCKET_NAME=dash-gtd-us-east-1-processed-data
    ports:
      - "8003:8003"
    volumes:
      - ./daily_exports:/app/daily_exports
  
  data-api-service:
    build:
      context: ./data_api_service
      dockerfile: Dockerfile
    environment:
      - SQL_SERVER=dash-gtd.database.windows.net
      - SQL_DATABASE=us_stock_options_db
      - SQL_USERNAME=dash_gtd
      - SQL_PASSWORD=wearethebest@69
      - SQL_DRIVER=ODBC Driver 18 for SQL Server
      - DJANGO_DEBUG=True
      - DJANGO_SECRET_KEY=django-insecure-k^c404!2*woj(h(+ek*e#=0sh^qrjw9y-g34m^*qy=^=!43v%^
    ports:
      - "8006:8006"
    networks:
      - stock_pipeline
    restart: unless-stopped

  monitoring:
    build:
      context: ./monitoring_service
      dockerfile: Dockerfile
    ports:
      - "8004:8004"
    networks:
      - stock_pipeline

  observability:
    build:
      context: ./observability_service
      dockerfile: Dockerfile
    ports:
      - "8005:8005"
    depends_on:
      - prometheus
      - grafana
    networks:
      - stock_pipeline

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092

    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions.sh", "--bootstrap-server", "kafka:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - stock_pipeline

  kafka-init:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - kafka
    entrypoint: ["/bin/bash", "-c"]
    command: |
      "
      for i in {1..30}; do
        kafka-broker-api-versions --bootstrap-server kafka:9092 && break
        echo 'Waiting for Kafka to be ready...'
        sleep 5
      done

      kafka-topics --create --topic fetch-trigger-daily            --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic fetch-trigger-15min            --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic fetch-trigger-historical       --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic fetch-trigger-options          --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
    
      kafka-topics --create --topic daily-data                     --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic 15min-data                     --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic options-data                   --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic historical-data                --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      
      kafka-topics --create --topic processed-daily-data           --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic processed-15min-data           --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic processed-options-data         --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic processed-historical-data      --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5

      kafka-topics --create --topic processed-file-daily-data      --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic processed-file-15min-data      --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic processed-file-options-data    --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5
      kafka-topics --create --topic processed-file-historical-data --bootstrap-server kafka:9092 --replication-factor 1 --partitions 5

      echo 'Kafka topics created!'
      "
    networks:
      - stock_pipeline

      
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - stock_pipeline

  minio:
    image: minio/minio:latest
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    networks:
      - stock_pipeline

  prometheus:
    image: prom/prometheus:v2.54.1
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - stock_pipeline
      - monitoring


  grafana:
    image: grafana/grafana:11.2.2
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - stock_pipeline
      - monitoring
  
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node_exporter
    ports:
      - "9100:9100"
    networks:
      - monitoring

volumes:
  mysql_data:
  influxdb_data:
  minio_data:
  grafana_data:

networks:
  stock_pipeline:
    driver: bridge
  monitoring:
    driver: bridge